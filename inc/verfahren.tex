\chapter{Hierarchisch-agglomerative Verfahren}

In diesem Kapitel sollen einzelne hierarchisch-agglomerative Verfahren untersucht werden, da sich Austausch- oder divisive Verfahren in der tatsächlichen Anwendung nicht bewährt haben. Der Grund hierfür ist die höhere Rechenzeit für Algorithmen der divisiven Verfahren in Verbindung mit dem Fehlen eines Nachweises, dass divisive Strategien präzisere Clusterlösungen liefern als agglomerative Verfahren. In der Praxis wird daher meist auf hierarchisch-agglomerative Verfahren zurückgegriffen \citep[Vgl.][S. ???]{Pedrycz.2010}.
Nach \citet[S.378]{Piegorsch.2015} kann der Austausch des zugrundeliegenden Verfahrens Fluch oder Segen sein: Die Verwendung eines anderen Verfahrens auf Basis der selben Daten zeigt interessante oder unerwartete Änderungen im Ergebnis der Clusteranalyse. Dies ist eine Beobachtung, die den zugrundeliegenden Prozess der Wissensschöpfung verbessern kann.

\section{Single Linkage-Verfahren}
Bei dem Single Linkage-Verfahren können sowohl Ähnlichkeits- als auch Distanzmaße verwendet werden. Dabei werden jeweils die zwei Cluster zusammengefasst, welche die kleinste Distanz zwischen sich aufweisen. Die Distanz entspricht dabei der kleinstmöglichen Distanz zwischen Objekten der verschiedenen Cluster. Nach dem Bilden eines neuen Clusters müssen die Distanzen jeweils neu ausgerechnet werden und die neue Minimaldistanz identifiziert werden.
Nach \citet[S.481]{Backhaus.2016} kann die neue Distanz vereinfacht mit folgender Formel errechnet werden:
\begin{equation}
	D(R;P+Q) = min\{D(R,P);D(R,Q)\}
\end{equation}
Wobei D(R,P) die Distanz zwischen den Objekten R und P, D(R,Q) die Distanz zwischen den Objekten R und Q und D(R;P+Q) die Distanz zwischen dem Objekt R und dem Cluster mit den Objekten P und Q ist.
Aufgrund des Vorgehens wird dieses Verfahren auch "Nearest-Neighbour-Verfahren" genannt \citep[Vgl.][S.231]{Eckey.2002}.
Dieses Verfahren ist ein kontrahierendes Verfahren. Zudem kann es genutzt werden um Ausreißer zu identifizieren \citep[Vgl.][S. 481-483]{Backhaus.2016}.
Dieses Verfahren erzeugt oft Cluster, die ziemlich diffus, langgestreckt und/oder unförmig sind \citep[Vgl.][S. 377]{Piegorsch.2015}.
Hierbei entsteht allerdings auch das Problem des Verkettungseffekts: Cluster werden zusammengefasst, die nur durch eine Brücke verbunden sind, im Raum aber deutlich separiert voneinander liegen. Dies kann zu eher heterogenen Clustern führen, wie in Abbildung \ref{pic:klemm19} gut zu erkennen ist \citep[Vgl.][S. 233]{Eckey.2002}. 

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=5cm]{pics/klemm19.png}
	\end{center}
	\caption{Der Chaining-Effekt nach Klemm}
	\label{pic:klemm19}
\end{figure}

Nach \citet[S. 416]{Clarke.2009} sind solche ungewöhnlichen Strukturen in der Natur allerdings durchaus üblich. Er zitiert: "...it's unclear in general wehether such properties are features or bugs." Der Einfluss solcher Fusionierungseigenschaften ist noch nicht vollständig untersucht.

\section{Complete Linkage-Verfahren}
Bei dem Complete Linkage-Verfahren können ebenfalls sowohl Ähnlichkeits- als auch Distanzmaße verwendet werden. Es werden jeweils die Cluster mit der geringsten Distanz zusammengefasst. Die Distanz berechnet sich allerdings anders als beim Single Linkage-Verfahren. Die Distanz zwischen Clustern entspricht hier nicht der kleinstmöglichen, sondern der größtmöglichen Distanz zwischen Objekten der verschiedenen Cluster. Auch hier muss nach jedem Zusammenfassen zweier Cluster die Distanzen jeweils neu ausgerechnet werden und die neue Maximaldistanz identifiziert werden.
Nach \citet[S.483]{Backhaus.2016} kann die neue Distanz vereinfacht mit folgender Formel errechnet werden:                                                                                                                                                                                                                                                                                                                                                               
\begin{equation}
	D(R;P+Q) = max\{D(R,P);D(R,Q)\}
\end{equation}
Aufgrund des Vorgehens wird dieses Verfahren auch "Furthest-Neighbour-Verfahren" genannt.
Dieses Verfahren ist ein dilatierendes Verfahren. Es eignet sich im Gegensatz zum Single Linkage- Verfahren nicht gut, um Ausreißer zu identifizieren, da es eher kleine Gruppen bildet \citep[Vgl.][S. 483/484]{Backhaus.2016}. Ein Problem der Orientierung an den maximal entfernten Objekten zweier Cluster stellt das Ausbleiben einer Fusionierung dar, selbst wenn die mittlere Distanz dieser zweier Objekte keine merkliche Erhöhung der Heterogenität im neu zu bildenden Cluster anzeigen würde \citep[Vgl.][S. 236]{Eckey.2002}.

\section{Average Linkage-Verfahren}
Das Average Linkage-Verfahren ist eine Modifikation vom Single und Complete Linkage-Verfahren. Dadurch wird versucht die Vorteile der beiden Verfahren zu kombinieren und die Nachteile zu eliminieren. Die Distanz berechnet sich dabei auf Grundlage einer Durchschnittsbildung von den Distanzen der vereinten Objekten.
Nach \citet[S. 264]{Bacher.2010} lässt sich der Mittelwert mithilfe von drei verschiedenen Verfahren berechnen:
\begin{enumerate}
	\item Average-Linkage
	\begin{equation}
		u_{(p+q),i} = \frac{u_{p,i} + u_{q,i}}{2}
	\end{equation}
	Hier findet eine einfache Durchschnittsbildung statt. \citet[S. 79]{Everitt.2011} beschreibt dieses Verfahren als relativ robust und dazu geneigt, Cluster mit geringen Varianzen zu fusionieren.
	\item Weighted-Average-Linkage
	\begin{equation}
		u_{(p+q),i} = \frac{u_{p,i}n_{p} + u_{q,i}n_{q}}{n_{p} + n_{q}}
	\end{equation}
	Hier werden Merkmalsausprägungen in kleinen Clustern höher gewichtet als Merkmalsausprägungen in großen Clustern \citep[Vgl.][S. 79]{Everitt.2011}.
	\item Within-Average-Linkage \colorbox{red}{selbe Formel wie Weighted-Average-Linkage? -> ja, weil sie sich nur minimal unterscheiden (zweite Formel S. 265 + Erklärung darunter. Deshalb Vorschlag: Wir nehmen hier nicht schon wieder Bacher, sondern verwenden eine andere gute Quelle ohne Within(könnte ich morgen machen), bleiben aber in der gleichen Notation wie Bacher, um Missverständnisse zu vermeiden)}
\end{enumerate}
Dabei bezeichnet $u_{p,i}$ die Unähnlichkeit zwischen den Clustern p und i, $u_{q,i}$ die Unähnlichkeit zwischen den Clustern q und i.
Dieses Verfahren ist ein konservatives Verfahren \citep[Vgl.][S. 264-266]{Bacher.2010}. 

\section{Centroid-Verfahren}
Das Centroid-Verfahren benötigt zur Berechnung quantitative Daten. Bei diesem Verfahren werden Clusterzentren gebildet, welche als Repräsentaten für den jeweiligen Cluster dienen. Dabei werden jeweils die Clusterpaare zusammengefasst, deren euklidische Distanz zum Clusterzentrum am geringsten ist. Nachdem die Clusterpaare zusammengefasst wurden muss das Clusterzentrum neu berechnet werden. Die ständige Neuberechnung des Clusterzentrums kann man umgehen, wenn man stattdessen die Unähnlichkeit der Objekte berechnet. Das Centroid-Verfahren ist ein konservatives Verfahren \citep[Vgl.][S. 285-289]{Bacher.2010}. Nach \citet[S. 79]{Everitt.2011} dominiert nach einer Fusionierung der größere Cluster von den ursprünglichen beiden.
\citet[S. 243]{Eckey.2002} weisen auf das Problem hin, dass die Bildung eher heterogener Cluster nicht auszuschließen ist, sofern die Clusterzentren nur nahe genug beieinander liegen.

\colorbox{red}{Reminder: Hier kommt auch das Problem mit den Umkehrungen/Inversionen für Centroid, Median, Ward}

\section{Median-Verfahren}
Das Median-Verfahren benötigt genauso, wie das Centroid-Verfahren quantitative Daten zur Berechnung. Ebenso werden die Clusterpaare zusammengefasst, deren euklidische Distanz zu dem Clusterzentrum minimal ist. Nach dem Zusammenfassen der Clusterpaare wird das neue Clusterzentrum errechnet. Dafür wird der Median aus den zwei alten Clusterzentren gebildet. Dies geschieht nach \citet[S. 286]{Bacher.2010} nach folgender Formel:
\begin{equation}
	x_{(p+q),j} = \frac{x_{pj} + x_{qj}}{2}
\end{equation}
Hier ist ebenso die ständige Neuberechnung der Clusterzentren nicht notwendig, wenn man stattdessen die Unähnlichkeit der Objekte berechnet. Dabei ist das Verschmelzungsschema ähnlich wie bei dem Average Linkage-Verfahren schwer zu interpretieren \citep[Vgl.][S. 285-289]{Bacher.2010}. Neu entstehende Cluster liegen in der Mitte der beiden ursprünglichen Cluster \citep[Vgl.][S. 79]{Everitt.2011}.
Im Gegensatz zum Centroid-Verfahren berücksichtigt das Median-Verfahren die Besetzungszahlen der beiden Cluster bei der Fusion nicht. Bei gleicher Objektanzahl können beide Verfahren allerdings als identisch angesehen werden \citep[Vgl.][S. 242]{Eckey.2002}.

\section{Ward-Verfahren}
Das Ward-Verfahren wird oft als sehr guter Clusteralgorithmus gesehen, da es im Vergleich zu anderen Verfahren sehr gute Partitionen findet und Objekte "richtig" den Clustern zuordnet. Dazu müssen jedoch folgende Bedingungen gegeben sein, da dieses Verfahren sehr sensibel reagiert: 

\begin{itemize}
	\item Verwendung eines geeigneten Distanzmaßes
	\item alle Variablen müssen metrisches Skalenniveau besitzen
	\item Ausreißer müssen vor Anwendung des Verfahrens eliminiert werden
	\item alle Variablen müssen unkorreliert sein
	\item die Elementanzahl sollte in jeder Gruppe als in etwa gleich groß erwartet werden
	\item die einzelnen Cluster müssen die gleiche Ausdehnung besitzen
\end{itemize}

Im Verfahren werden jeweils die Objekte zusammengefasst, welche die Streuung innerhalb eines Cluster möglichst wenig erhöhen. Die Cluster werden also so gebildet, dass sie möglichst homogen sind. Ward neigt jedoch dazu, möglichst gleich große Cluster zu bilden und langgestreckte Cluster bzw. kleine Cluster mit wenigen Objekten nicht zu erkennen \citep[Vgl.][S. 484]{Backhaus.2016}. \\

\section{Zusammenfassung der behandelten Verfahren}
\citet[S. 489]{Backhaus.2016} geben einen guten Überblick über alle hier behandelten Verfahren: \\
\begin{tabular}{|l|l|l|l|p{3.7cm}|}
	\hline
	\rowcolor{babyblueeyes}Verfahren & Eigenschaft & Monoton? & Proximitätsmaße & Bemerkungen \\ \hline
	\rowcolor{beaublue}Single Linkage & kontrahierend & ja & alle & neigt zur Kettenbildung \\ \hline
	\rowcolor{beaublue}Complete Linkage & dilatierend & ja & alle & neigt zu kleinen Gruppen \\ \hline	
	\rowcolor{beaublue}Average Linkage & konservativ & ja & alle & \\ \hline
	\rowcolor{beaublue}Centroid & konservativ & nein & Distanzmaße & \\ \hline
	\rowcolor{beaublue}Median & konservativ & nein & Distanzmaße & \\ \hline
	\rowcolor{beaublue}WARD & konservativ & ja & Distanzmaße & bildet etwa gleich große Gruppen \\ \hline
\end{tabular}
